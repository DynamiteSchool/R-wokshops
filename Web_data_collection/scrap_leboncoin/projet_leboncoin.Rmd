---
title: "Scraping leboncoin"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

```{r, message=FALSE}

library(httr)
library(rvest)
library(tidyverse)
library(knitr) # Just to have nice tables in the html document... -> function 'kable'
# And for geocoding:
# devtools::install_github(repo = 'rCarto/photon')  
library(photon)
```

# Manage to read leboncoin pages

If you try to read a page in Leboncoin using the `read_html()` function directly you'll likely get a 403 error message, meaning you have been denied the access. Send a query as if it was a regular query sent **from your browser**.

Open your browser **Inspector** -> go to the Network tab and them go select the part of the answer that corresponds to the html part of the answer. Then have a look at the **headers** sent along with your query. We'll use 3 info items here and add them to our query :

- User-Agent
- Accept (accepted formats for the answer)
- Accept-Language (accepted languages for the answer)

```{r GET_leboncoin_pages}
go_GET <- function(url){
  result=GET(url,
             add_headers(
               "User-Agent" = "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:62.0) Gecko/20100101 Firefox/62.0",
               "Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
               "Accept-Language"="fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3"))
  return(result)
}
go_GET("https://www.leboncoin.fr/ventes_immobilieres/offres/rhone_alpes/rhone/")
```


# Scrape all ads in the real estate category in leboncoin

... for now, just for 1 department: RhÃ´ne.

## Get links to all ads

Get the total number of ads and deduce the number of pages to scrape (35 ads are displayed per page).

```{r}
url_base <- "https://www.leboncoin.fr/ventes_immobilieres/offres/rhone_alpes/rhone/"
url_base_raw <- go_GET(url_base)
html_base <- read_html(url_base_raw)

nb_links <- html_base  %>% 
  html_nodes("._2ilNG") %>%
  html_text() %>% 
  first() %>% 
  str_replace(" ","") %>% 
  as.numeric()
nb_pages=ceiling(nb_links/35)


pages=c(url_base,
        str_c(url_base,"p-",2:nb_pages))
pages[1:5]
```
Now we have the urls of all the pages we have to scrape to get links to all ads (vector `pages`).

## For each page, get link to individual ads

Definition of function `ads_by_page()` which takes a **page listing ads as an input and returns all ads' urls** as output.

I added some **random waiting time** to each call to `ads_by_page()` of 1 to 5 seconds.

```{r ads_by_page}
ads_by_page <- function(page){
  Sys.sleep(runif(1,1,5))
  my_html <- read_html(go_GET(page))
  links <- my_html %>%
      html_nodes(".clearfix") %>% 
      html_attr("href") %>% 
      na.omit()  
  tib <- tibble(urls=str_c("https://www.leboncoin.fr",links)) 
  return(tib)
}  
ads_by_page(pages[1])
```

Now **apply iteratively** function `ads_by_page()` to all pages' urls listed in `pages`.

And I didn't actually do it on all `r length(pages)` pages but only on 3 of them, to show you the principle!

```{r tib_ads_urls}
tib_ads_urls <- map(pages[1:3],safely(ads_by_page)) %>%
  map("result") %>% 
  bind_rows()
```

## For each ad, get info

Define function `ad_info()`, which takes **an ad's url** as an input and returns, as an output, a **tibble** with information regarding

- `url`: the ads' urls
- `title`: their titles,
- `type`: the type of property
- `surface`: the surface of the property
- `rooms`: the number of rooms
- `GHG`: Greenhouse gas emission category
- `energy_class`: Energy class category,
- `location`: Location of the property

I added some **random waiting time** to each call to `ad_info()` of 1 to 5 seconds.

```{r ad_info}
ad_info <- function(ad){
    Sys.sleep(runif(1,1,5))
    html_ad <- read_html(go_GET(ad))
    title <- html_ad %>% 
      html_nodes("._1KQme") %>% 
      html_text()
    criteria <- 
      tibble(name= html_ad %>% html_nodes("._3-hZF") %>% html_text(),
             value=html_ad %>% html_nodes("._3Jxf3") %>% html_text()) 
    f=function(x){if(length(x)==0){x=NA};return(x)}
    type    <- filter(criteria, name=="Type de bien")$value %>% f()
    surface <- filter(criteria, name=="Surface")$value %>%
      str_extract("^\\d*") %>% f()
    rooms   <- filter(criteria, str_detect(name,"Pi.ces"))$value %>% 
      as.numeric() %>% f()
   
    price <- html_ad %>% 
      html_nodes(".eVLNz") %>% 
      html_text() %>% 
      first() %>% 
      str_replace_all("[^0-9]","") %>% 
      as.numeric() 
    GHG <- html_ad %>% 
      html_nodes("._2BhIP") %>% 
      html_text() %>% 
      first()
    energy_class <-html_ad %>% 
      html_nodes("._15MMC") %>% 
      html_text() %>% 
      first()
    location <- html_ad %>% 
      html_nodes("._1aCZv") %>% 
      html_text() %>% 
      str_replace("Voir sur la carte","")
    ## Geocoding
    #
    #zipcode <- str_extract(location,"\\d+") 
    #city <- str_extract(location,"[A-Za-z- ]+")
    # url <- str_c("https://geocode.xyz/",zipcode,"+",city,"?json=1&region=FR")
    # raw_json <- GET(url)
    # geocode <- content(raw_json,as="parsed")
    # latitude <- geocode$latt
    # longitude <- geocode$longt
    coord_table=photon::geocode(location)
    latitude=coord_table$lat[1]
    longitude=coord_table$lon[1]
    tib_ad=bind_cols(urls=ad,
                     title=title,
                     price=price,
                     type=type,
                     surface=surface,
                     rooms=rooms,
                     GHG=GHG,
                     energy_class=energy_class,
                     location=location,
                     latitude=latitude,
                     longitude=longitude)
    return(tib_ad)
}
ad_info(tib_ads_urls$urls[1]) %>% kable()
```

Please note that while during the course we had geocoded using the geocode.xyz API, this might not be optimal for geocoding with R. This choice was due to the fact that we wanted to show you **how to use an API in a direct query**. You can also geocode using other APIs with API clients (see for instance function `geocode()` in package `photon`, which is the solution we finally used in this document).

Now **apply iteratively this function `ad_info()`** to all ads in `tib_ads_urls`, using `purrr` iteration.

I actually did not do it on all ads but just on 20 of them to show you the principle!

```{r tib_ads}
tmp=Sys.time()
tib_ads <- map(tib_ads_urls$urls[1:20],
                  safely(ad_info)) %>% 
     map("result") %>% bind_rows()
time_for_20_ads <- Sys.time()-tmp
tib_ads %>% kable()
```

For 20 ads, it took us about 1.5 minutes to get the data so if we would like to do this on all ads (~14000 ads) then it would take along time (about 18 hours...)!
